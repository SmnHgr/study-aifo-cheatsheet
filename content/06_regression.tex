\section{Linear Regression}
a simple method to analyse data \\

\begin{itemize}
    \item Only considers a linear relationship between input and output
    \item In the simplest case, $x$ and $y$ are scalars and the linear model therefore has only two free parameters
    \item The goal is to identify $a$ (slope) and $b$ (intercept) for which the linear model best explains the data
\end{itemize}
\begin{center}
    $\hat{y_i} = ax_i + b$
\end{center}

Other examples

Squared: $\hat{y_i} = w_2 * x_i^2 + w_1 * x_i + w_0$\\
Cubic: $\hat{y_i} = w_3 * x_i^3 + w_2 * x_i^2 + w_1 * x_i + w_0$\\

\textbf{Applications}
\begin{itemize}
    \item \textcolor{blue}{Interpretation} has some input an effect on the output, eg. Is there a relationship between smoking cigaretts and the risk of lung cancer?
    \item \textcolor{blue}{Prediction} Given some sensor data like oil pressure, temperature ..., eg. a model could predict (and thereby hopefully prevent) an engine failure.
\end{itemize}

\subsection{Model}
In ML, we use the term \textcolor{blue}{model} for any mathematical function that explains the data\\
\begin{center}
    $y_i \approx f(x_i)$\\
    $y_i = f(x_i) + \epsilon_i$\\
\end{center}
where $\epsilon_i$ is unexplained noise. It is often assumed that $\epsilon_i$ follows a normal distribution.\\
Instead of approximating $y_i$, we calculate an \textcolor{blue}{estimate} $\hat{y_i}$ (y hat) of the usually unknown $y_i$: \\
\begin{center}
    $\hat{y_i} = f(x)$
\end{center}



\subsection{Mean Squared Error (MSE)}
\begin{itemize}
    \item Loss we want to minimize
    \item Usually divided by 2
\end{itemize}
\begin{center}
    $\hat{y_i} = ax_i + b$\\
    $e_i = y_i - \hat{y_i}$ \\
    The difference $e_i$, called residual\\
    $MSE = \frac{1}{2N} * \displaystyle\sum_{i = 1}^{N} e_i^2$\\
    $MSE = \frac{1}{2N} * \displaystyle\sum_{i = 1}^{N} (y_i - \hat{y_i})^2$\\
    $MSE = \frac{1}{2N} * \large\displaystyle\sum_{i = 1}^{N}({y_i} - (a*x_i + b))^2$
\end{center}

\subsection{Correlation \sout{and Causality}}
\begin{itemize}
    \item Correlation is not causality
    \item Correlation refers to the degree to which a pair of variables are \textcolor{blue}{linearly related}
    \item Linear regression is a tool to detect correlations between two or more variables
    \item Correlation can be quantified using the Pearson correlation coefficient
\end{itemize}
