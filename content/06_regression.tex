\section{Linear Regression (Model)}
a simple method to analyse data \\

\begin{itemize}
    \item Only considers a linear relationship between input and output
    \item In the simplest case, $x$ (Feature) and $y$ are scalars and the linear model therefore has only \textcolor{blue}{two free parameters}
    \item The goal is to identify $a$ (slope/Steigung) and $b$ (intercept/offset) for which the linear model best explains the data
\end{itemize}
\begin{center}
    $\hat{y_i} = ax_i + b$
\end{center}

Other examples

\textcolor{blue}{Squared} $\hat{y_i} = w_2 * x_i^2 + w_1 * x_i + w_0$\\
\textcolor{blue}{Cubic} $\hat{y_i} = w_3 * x_i^3 + w_2 * x_i^2 + w_1 * x_i + w_0$\\

\textbf{Applications}
\begin{itemize}
    \item \textcolor{blue}{Interpretation} has some input an effect on the output, eg. Is there a relationship between smoking cigaretts and the risk of lung cancer?
    \item \textcolor{blue}{Prediction} Given some sensor data like oil pressure, temperature ..., eg. a model could predict (and thereby hopefully prevent) an engine failure.
\end{itemize}

\subsection{Model}
\textcolor{blue}{model} any mathematical function that explains the data\\

\begin{center}
    $y_i \approx f(x_i)$\\
    $y_i = f(x_i) + \epsilon_i$\\
\end{center}
where $\epsilon_i$ is unexplained noise. It is often assumed that $\epsilon_i$ follows a normal distribution.\\
Instead of approximating $y_i$, we calculate an \textcolor{blue}{estimate} $\hat{y_i}$ (y hat) of the usually unknown $y_i$: \\
\begin{center}
    $\hat{y_i} = f(x)$
\end{center}



\subsection{Mean Squared Error (MSE) - Loss}
\begin{itemize}
    \item Loss we want to minimize
    \item Usually divided by 2
    \item Parameters sind die Daten (X und Y) sowie die Parameters des Models ($a$ und $b$)
    \item $MSE = f (X, Y, a, b)$
\end{itemize}
\begin{center}
    $\hat{y} =$ Sch√§tzung von $y$

    $\hat{y_i} = ax_i + b$

    $e_i =$ difference/\textcolor{blue}{residual} (Restfehler)

    $e_i = y_i - \hat{y_i}$

    $MSE = \frac{1}{2N} * \displaystyle\sum_{i = 1}^{N} e_i^2$

    $MSE = \frac{1}{2N} * \displaystyle\sum_{i = 1}^{N} (y_i - \hat{y_i})^2$

    $MSE = \frac{1}{2N} * \large\displaystyle\sum_{i = 1}^{N}({y_i} - (a*x_i + b))^2$
\end{center}

\subsection{Correlation \sout{and Causality}}
\begin{itemize}
    \item Correlation is not causality
    \item Correlation refers to the degree to which a pair of variables are \textcolor{blue}{linearly related}
    \item Linear regression is a tool to detect correlations between two or more variables
    \item Correlation can be quantified using the Pearson correlation coefficient
\end{itemize}
