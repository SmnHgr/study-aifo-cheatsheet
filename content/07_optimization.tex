\section{Optimization}
\begin{itemize}
    \item Training or learning often suggests an algorithm performing some sort of optimization
    \item It is the problem of finding a set of inputs to an objective function that results in a maximum or minimum function evaluation
    \item In our examples \textcolor{blue}{the objective is to minimize the loss function}
\end{itemize}

\subsection{Gradient Descent}
At any location [a,b] we look at the error-gradient in the neighbourhood of [a,b] and move a (small) step in the direction where the error shrinks the most.
By repeating this procedure, we will eventually arrive at the location where the error is smallest. \\

\textcolor{red}{Gradient-based methods only work if we can express a Loss function as a differentiable function} \\

\begin{itemize}
    \item \textcolor{blue}{Iterative Method/Procedure}
    \item Each iteration, the model parameters are updated such as that the Loss (MSE) is reduced
    \item Move along a trajectory which includes fewer points
    \item At each point of the trajectory we evaluate the gradient of the error function
    \item At each iteration, we would have to iterate over all $N = 1'000$ points to calculate the gradient of the loss function.
\end{itemize}

\textbf{MSE formula}
\begin{center}
    $E = \frac{1}{2N} * \large\displaystyle\sum_{i = 1}^{N}(\hat{y_i} - (a*x_i + b))^2$
\end{center}

\textbf{Gradient of E}

Calculate these two partial derivatives

\begin{center}
    $\frac{\partial E}{\partial a} = \frac{1}{N}\sum_{i=1}^N (y_i - (a \cdot x_i + b)) \cdot -x_i$
\end{center}

\begin{center}
    $\frac{\partial E}{\partial b} = \frac{1}{N}\sum_{i=1}^N (y_i - (a \cdot x_i + b)) \cdot -1$
\end{center}

\begin{center}
    $
     \textrm{Gradient of E} = \begin{bmatrix}
                                  \frac{\partial E}{\partial a} \\
                                  \frac{\partial E}{\partial b} \\
     \end{bmatrix}
     =\begin{bmatrix}
          \frac{1}{N}\sum_{i=1}^N (y_i - (a \cdot x_i + b))(-x_i) \\
          \frac{1}{N}\sum_{i=1}^N (y_i - (a \cdot x_i + b))(-1) \\
     \end{bmatrix}
    $
\end{center}

\textbf{Update Rule}

\begin{center}
    $
    \begin{bmatrix}
        a \\
        b \\
    \end{bmatrix}_{t+1}
    =    \begin{bmatrix}
             a \\
             b \\
    \end{bmatrix}_{t} - \alpha
    \begin{bmatrix}
        \frac{\partial E}{\partial a} \\
        \frac{\partial E}{\partial b} \\
    \end{bmatrix} \Big\rvert _{\begin{bmatrix}
                                   a \\
                                   b \\
    \end{bmatrix}_{t} }
    $
\end{center}

The algorithm starts at some initial position $[a, b]_{t0}$. \\
Then, the update rule is applied repeatedly.
The number of update steps needed to come ``close enough'' to the minimum depends on the problem.

\subsection{Stochastic Gradient Descent (SGD)}
we do not need the exact gradient to find a trajectory toward the minimum. Instead, at each iteration we can randomly pick a few datapoints and use them to calculate an approximation of the gradient. \\

\begin{itemize}
    \item At each iteration, the gradient is calculated on a (randomly selected) subset of the data
    \item For a fixed learning rate, SGD does not converge
\end{itemize}
\vspace{10pt}
\textbf{Update Rule}

$n = 1$
\begin{center}
    $
    \begin{bmatrix}
        a \\
        b \\
    \end{bmatrix}_{t+1}
    =    \begin{bmatrix}
             a \\
             b \\
    \end{bmatrix}_{t} - \alpha
    \begin{bmatrix}
        \frac{\partial E_j}{\partial a} \\
        \frac{\partial E_j}{\partial b} \\
    \end{bmatrix} \Big\rvert _{\begin{bmatrix}
                                   a \\
                                   b \\
    \end{bmatrix}_{t} }
    $
\end{center}
\vspace{10pt}

\textbf{Batch sizes}

Increasing the batch-size will reduce the variance of the gradient estimation

\begin{itemize}
    \item \textcolor{blue}{Mini-batches} $1<n<N$, often size $n=32$ or $n=64$ are used
    \item \textcolor{blue}{SGD} batch-size $n=1$ yields a very noisy gradient
    \item \textcolor{blue}{Gradient Descent} batch-size $n=N$ is expensive to calculate
\end{itemize}
\vspace{10pt}
\textbf{(simulated) Annealed SGD}

\begin{itemize}
    \item It starts with a large \textcolor{blue}{learning rate alpha} $\alpha$ (SchrittgrÃ¶sse) is reduces over time
    \item There are different options (called schedules) how to reduce alpha over time (e.g. \textcolor{blue}{exponential decay})
    \item A fixed learning rate $\alpha$ does not converge. The algorithm keeps fluctuating around the minimum. Annealed SGD solves this appearent contradiction by adapting the learning rate.
    \item SGD is dealing with only a single datum at each iteration. This is very inefficient and rarely used.
\end{itemize}
